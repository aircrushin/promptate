{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTJK9gzv7-aF"
      },
      "source": [
        "# add dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPZOP_Kf8_XU",
        "outputId": "d58755df-5e8c-4fd0-a064-d7a258c50b96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-1.3.9-py3-none-any.whl (221 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, h11, dill, tiktoken, multiprocess, httpcore, httpx, openai, datasets\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.15.0 dill-0.3.7 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 multiprocess-0.70.15 openai-1.3.9 pyarrow-hotfix-0.6 tiktoken-0.5.2\n"
          ]
        }
      ],
      "source": [
        "%pip install datasets tiktoken openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFD-Kyp09fq0",
        "outputId": "88608d1e-491c-40e4-951b-af5ad7896de3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9Cd-wePDHK2"
      },
      "source": [
        "# explore data and cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGEVodWv9K2k",
        "outputId": "54d34d7a-e578-4dc1-9a23-a2a281d9176a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num examples: 153\n",
            "First example:\n",
            "{'role': 'system', 'content': 'you are a strong and helpful assistant to create prompt from a single or several words'}\n",
            "{'role': 'user', 'content': 'Linux Terminal'}\n",
            "{'role': 'assistant', 'content': 'I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd'}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import tiktoken # for token counting\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "data_path = \"/content/training_data.jsonl\"\n",
        "\n",
        "# Load the dataset\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# Initial dataset stats\n",
        "print(\"Num examples:\", len(dataset))\n",
        "print(\"First example:\")\n",
        "for message in dataset[0][\"messages\"]:\n",
        "    print(message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scl0xzENAPQd",
        "outputId": "63f4ca45-6c59-4b06-9e0a-39b0a91a17b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No errors found\n"
          ]
        }
      ],
      "source": [
        "# Format error checks\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        function_call = message.get(\"function_call\", None)\n",
        "\n",
        "        if (not content and not function_call) or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ML94XaJ6AWXV"
      },
      "outputs": [],
      "source": [
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# not exact!\n",
        "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpkjfmaDAdkG",
        "outputId": "bbbb40b7-00ef-4f89-b91e-9f5faa30bd00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num examples missing system message: 0\n",
            "Num examples missing user message: 0\n",
            "\n",
            "#### Distribution of num_messages_per_example:\n",
            "min / max: 3, 3\n",
            "mean / median: 3.0, 3.0\n",
            "p5 / p95: 3.0, 3.0\n",
            "\n",
            "#### Distribution of num_total_tokens_per_example:\n",
            "min / max: 68, 403\n",
            "mean / median: 126.43137254901961, 119.0\n",
            "p5 / p95: 99.2, 156.80000000000007\n",
            "\n",
            "#### Distribution of num_assistant_tokens_per_example:\n",
            "min / max: 33, 366\n",
            "mean / median: 92.27450980392157, 85.0\n",
            "p5 / p95: 65.2, 123.2000000000001\n",
            "\n",
            "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
          ]
        }
      ],
      "source": [
        "# Warnings and tokens counts\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_messages_per_example\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "n_too_long = sum(l > 4096 for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrgeL6PSAwaE",
        "outputId": "cd493679-bcf5-48d2-872c-69a171493f3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset has ~19344 tokens that will be charged for during training\n",
            "By default, you'll train for 3 epochs on this dataset\n",
            "By default, you'll be charged for ~58032 tokens\n"
          ]
        }
      ],
      "source": [
        "# Pricing and default n_epochs estimate\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "TARGET_EPOCHS = 3\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "MIN_DEFAULT_EPOCHS = 1\n",
        "MAX_DEFAULT_EPOCHS = 25\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1imA9WZbDUN9"
      },
      "source": [
        "## 500k gpt-3.5-turbo training tokens = $4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGc6pwofCOxd"
      },
      "source": [
        "# let's start finetune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Lk-yHksuA_8H"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import openai\n",
        "import os\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "#OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
        "OPENAI_API_KEY = \"sess-7KZWK9vWwzg81K51QMbuP30wEA7OP7jo7OpCdIGw\"\n",
        "openai.api_key = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Rp5eOOROBceu"
      },
      "outputs": [],
      "source": [
        "training_file_name = \"/content/training_data.jsonl\"\n",
        "\n",
        "validation_file_name = \"/content/validation_data.jsonl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSX-bYSiCGdn",
        "outputId": "a2e4ee57-4af7-484d-b85f-4e425c512cca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training file ID: file-Sc2HsBlnl2XxSBvZlP4i0OGx\n",
            "Validation file ID: file-ntF7rttTzS4HilmL2ydKxJAv\n"
          ]
        }
      ],
      "source": [
        "with open(training_file_name, \"rb\") as training_fd:\n",
        "    training_response = openai.files.create(\n",
        "        file=training_fd, purpose=\"fine-tune\"\n",
        "    )\n",
        "\n",
        "training_file_id = training_response.id\n",
        "\n",
        "with open(validation_file_name, \"rb\") as validation_fd:\n",
        "    validation_response = openai.files.create(\n",
        "        file=validation_fd, purpose=\"fine-tune\"\n",
        "    )\n",
        "validation_file_id = validation_response.id\n",
        "\n",
        "print(\"Training file ID:\", training_file_id)\n",
        "print(\"Validation file ID:\", validation_file_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo4pnsytCZNO"
      },
      "outputs": [],
      "source": [
        "response = openai.fine_tuning.jobs.create(\n",
        "    training_file=training_file_id,\n",
        "    validation_file=validation_file_id,\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    suffix=\"recipe-ner\",\n",
        ")\n",
        "\n",
        "job_id = response.id\n",
        "\n",
        "print(\"Job ID:\", response.id)\n",
        "print(\"Status:\", response.status)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97J4ecFXENLu"
      },
      "outputs": [],
      "source": [
        "response = openai.fine_tuning.jobs.retrieve(job_id)\n",
        "\n",
        "print(\"Job ID:\", response.id)\n",
        "print(\"Status:\", response.status)\n",
        "print(\"Trained Tokens:\", response.trained_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqY4OM_wEQtv"
      },
      "outputs": [],
      "source": [
        "response = openai.fine_tuning.jobs.list_events(job_id)\n",
        "\n",
        "events = response.data\n",
        "events.reverse()\n",
        "\n",
        "for event in events:\n",
        "    print(event.message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncoIvuRoEURn"
      },
      "outputs": [],
      "source": [
        "response = openai.fine_tuning.jobs.retrieve(job_id)\n",
        "fine_tuned_model_id = response.fine_tuned_model\n",
        "\n",
        "if fine_tuned_model_id is None:\n",
        "    raise RuntimeError(\"Fine-tuned model ID not found. Your job has likely not been completed yet.\")\n",
        "\n",
        "print(\"Fine-tuned model ID:\", fine_tuned_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71DJ00EFEbit"
      },
      "outputs": [],
      "source": [
        "test_df = recipe_df.loc[201:300]\n",
        "test_row = test_df.iloc[0]\n",
        "test_messages = []\n",
        "test_messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "user_message = create_user_message(test_row)\n",
        "test_messages.append({\"role\": \"user\", \"content\": create_user_message(test_row)})\n",
        "\n",
        "pprint(test_messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRslF4nfEdEe"
      },
      "outputs": [],
      "source": [
        "response = openai.chat.completions.create(\n",
        "    model=fine_tuned_model_id, messages=test_messages, temperature=0, max_tokens=500\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
